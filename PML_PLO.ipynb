{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRmX8cglaReN"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import VarianceThreshold # Feature selector\n",
        "from sklearn.pipeline import Pipeline # For setting up pipeline\n",
        "# Various pre-processing steps\n",
        "from sklearn.preprocessing import Normalizer, StandardScaler, MinMaxScaler, PowerTransformer, MaxAbsScaler, LabelEncoder\n",
        "from sklearn.model_selection import GridSearchCV # For optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose a small number of different machine learning algorithms and hyperparameters, along with sensible value ranges, and additional ML pipeline components. This could include, for example, a feature imputation step, a feature recoding step, and an ensembling or stacking step in addition to the learning algorithm. If those components have hyperparameters, you must choose ranges and tune them as well. In addition, your pipeline should allow the hyperparameter optimization to turn individual components on and off, e.g. use a one-hot-encoding or not.\n",
        "\n",
        "You can use implementations of AutoML systems (e.g. auto-sklearn), scientific papers, or the documentation of the library you are using to determine the hyperparameters to tune and the value ranges. Note that there is not only a single way to do this, but define a reasonable space (e.g. don't include whether to turn on debug output or random forests with 1,000,000 trees). Your ML pipeline needs to be reasonably complex, i.e. at least three components.\n",
        "\n",
        "Determine the best ML pipeline. Make sure to optimize the entire ML pipeline, not individual parts individually. Choose a suitable hyperparameter optimizer; you could also use several and e.g. compare the results achieved by random search and Bayesian optimization. Make sure that the way you evaluate this avoids bias and overfitting. You could use statistical tests to make this determination."
      ],
      "metadata": {
        "id": "mvob_0hsbCvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import models, packages\n",
        "from sklearn import linear_model, ensemble\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn import model_selection\n",
        "import numpy\n",
        "!pip install pipelinehelper\n",
        "\n",
        "#models to compare\n",
        "models = [linear_model.RidgeClassifier(), ensemble.BaggingClassifier(), ensemble.RandomForestClassifier()]\n",
        "\n",
        "pipe = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('onehot', OneHotEncoder()),\n",
        "    ('selector', VarianceThreshold()),\n",
        "    ('estimator', KNeighborsClassifier())\n",
        "])\n",
        "\n",
        "parameters = {\"scaler\":[StandardScaler(), MinMaxScaler(), Normalizer(), MaxAbsScaler(), \"passthrough\"],\n",
        "              \"onehot\":[OneHotEncoder(), \"passthrough\"],\n",
        "              \"selector\": [VarianceThreshold(), \"passthrough\"],\n",
        "                {\n",
        "                    'estimator':[KNeighborsClassifier()],\n",
        "                    'estimator__n_neighbors': [1, 5, 10],\n",
        "                    'estimator__algorithm ' :  ['ball_tree', 'kd_tree', 'brute'],\n",
        "                },\n",
        "                {\n",
        "                    'estimator':[RidgeClassifier()],\n",
        "                    'estimator__alpha':[1.0, 1.1, 2.0],\n",
        "                    'tol':[0.001, 0.01. 0.1],\n",
        "                    'solver': [\"svd\", \"cholesky\", \"lsqr\", \"sparse_cg\"],\n",
        "                },\n",
        "                {\n",
        "                    'estimator': [DecisionTreeClassifier()],\n",
        "                    'estimator__max_depth': [1,2,3,4,5],\n",
        "                    'estimator__max_features': [None, \"auto\", \"sqrt\", \"log2\"],\n",
        "                },\n",
        "}\n"
      ],
      "metadata": {
        "id": "bqFt3YeWaeiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''from pipelinehelper import PipelineHelper\n",
        "\n",
        "pipe = Pipeline([\n",
        "    ('scaler', PipelineHelper([\n",
        "        ('std', StandardScaler()),\n",
        "        ('max', MaxAbsScaler()),\n",
        "    ])),\n",
        "    ('classifier', PipelineHelper([\n",
        "        ('svm', LinearSVC()),\n",
        "        ('rf', RandomForestClassifier()),\n",
        "    ])),\n",
        "])\n",
        "\n",
        "params = {\n",
        "    'scaler__selected_model': pipe.named_steps['scaler'].generate({\n",
        "        'std__with_mean': [True, False],\n",
        "        'std__with_std': [True, False],\n",
        "        'max__copy': [True],  # just for displaying\n",
        "    }),\n",
        "    'classifier__selected_model': pipe.named_steps['classifier'].generate({\n",
        "        'svm__C': [0.1, 1.0],\n",
        "        'rf__n_estimators': [100, 20],\n",
        "    })\n",
        "}'''"
      ],
      "metadata": {
        "id": "uqke0ELZi00F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}